{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1038d2d",
   "metadata": {},
   "source": [
    "# Tutorial for Boundless DAS (v1.0)\n",
    "\n",
    "Author: Zhengxuan Wu (wuzhengx@stanford.edu)  \n",
    "Last Update: 06/17/2023\n",
    "\n",
    "## *Setups and Analysis, Federated Model Steering and Alignment Sharing*\n",
    "\n",
    "### Introduction\n",
    "We notice that we used an internal version (Stanford personales only) of Alpaca-7B model in the paper, which is not publicly avaliable. This greatly reduces the reproducibility of this paper. We thus provide this script to fetch an example model from huggingface model hub, and setup the training environment + verification of a single datapoint. \n",
    "\n",
    "Note that we do not hookup this script for our training script to that you can load specialized models later on.\n",
    "\n",
    "### What does this file cover?\n",
    "- Download a HF model.\n",
    "- Cast it into an alignable version (without any processing, simply downcasting).\n",
    "- Reproduce one example from the paper about boundless DAS training.\n",
    "- **A guide to share rotation matrix with others.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2de93390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    set_seed,\n",
    "    AutoConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "import seaborn as sns\n",
    "from datasets import Dataset\n",
    "import os, random, argparse, sys, torch\n",
    "from models.configuration_alignable_model import AlignableLlamaConfig\n",
    "from counterfactual_datasets.price_tagging_game import factual_sampler, bound_alignment_sampler, lower_bound_alignment_example_sampler\n",
    "from trainer import Aligner, CACHE_DIR\n",
    "import counterfactual_datasets.price_tagging_game as price_tagging_game\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from models.modelings_alignable import AutoAlignableModel\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_info()\n",
    "logger = logging.get_logger(\"transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3340897",
   "metadata": {},
   "source": [
    "#### Step 1: Load a HF model, Verify Its Task Performance, and Save It in the Disk for Alignment\n",
    "This is **super important** that is verifying the task performance! Sometimes, a little bug in the tokenizer (e.g., how SOS and EOS tokens are formulated) can largely change your expectation in performance. This is simply as the result of supervised tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b7d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ONCE_ALREADY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c034c6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [00:42<00:00, 14.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING: THIS NEEDS TO BE GOOD!] prealign task accuracy: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not RUN_ONCE_ALREADY:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"sharpbai/alpaca-7b-merged\",\n",
    "        cache_dir=\"../.cache_dir/\"\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"sharpbai/alpaca-7b-merged\",\n",
    "        cache_dir=\"../.cache_dir/\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    _ = model.to(\"cuda\")\n",
    "\n",
    "    raw_prealign = factual_sampler(\n",
    "        tokenizer,\n",
    "        5000,\n",
    "        game=\"pricing_tag\"\n",
    "    )\n",
    "    prealign_dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"input_ids\": raw_prealign[0], \n",
    "            \"labels\": raw_prealign[1],\n",
    "        }\n",
    "    ).with_format(\"torch\")\n",
    "    prealign_dataloader = DataLoader(\n",
    "        prealign_dataset, batch_size=8\n",
    "    )\n",
    "\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, inputs in enumerate(tqdm(prealign_dataloader)):\n",
    "            for k, v in inputs.items():\n",
    "                if v is not None and isinstance(v, torch.Tensor):\n",
    "                    inputs[k] = v.to(model.device)\n",
    "\n",
    "            # aligning forward!\n",
    "            outputs = model(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                labels=inputs['labels'],\n",
    "            )\n",
    "\n",
    "            actual_test_labels = inputs['labels'][:, -1]\n",
    "            pred_test_labels = torch.argmax(outputs.logits[:, -1], dim=-1)\n",
    "\n",
    "            correct_labels = (actual_test_labels==pred_test_labels)\n",
    "\n",
    "            total_count += len(correct_labels)\n",
    "            correct_count += correct_labels.sum().tolist()\n",
    "    current_acc = round(correct_count/total_count, 2)\n",
    "    print(f\"[WARNING: THIS NEEDS TO BE GOOD!] prealign task accuracy: {current_acc}\")\n",
    "    \n",
    "    # Now, we save this model to a separate folder for the alignment script to load. \n",
    "    # If you have your own model with your own dataset, you can follow the rest of \n",
    "    # steps here to create a DAS config file.\n",
    "    tokenizer.save_pretrained(\"../alpaca_7b/\")\n",
    "    model.save_pretrained(\"../alpaca_7b/\")\n",
    "\n",
    "    # we create a alignment config here. you want to change to your settings later on.\n",
    "    das_config = AlignableLlamaConfig()\n",
    "    das_config.save_pretrained(\"../alpaca_7b/das_config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7bd5c6",
   "metadata": {},
   "source": [
    "**===== STOP HERE AND READ =====**\n",
    "\n",
    "Now, you have the model to align! And if your code above run without an error, it probably means you have all the env stuff set up. Great! So, ...\n",
    "\n",
    "If your purpose is to run alignment script in a separate process, please read the `run_alignment.py` and you can close this notebook now. If you want to see example code about how to load alignable model, how to run trainer in a notebook, you can continue to read this notebook. We recommend you run the `*py` script, and the notebook is more like a playground instead.\n",
    "\n",
    "**===== STOP HERE AND READ =====**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76a697e",
   "metadata": {},
   "source": [
    "#### Step 2: Reproducing One Boundless DAS Result from Our Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8591674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "###################\n",
    "# data loaders\n",
    "###################\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"../alpaca_7b/\",\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "raw_data = bound_alignment_sampler(\n",
    "    tokenizer,\n",
    "    10000,\n",
    "    [lower_bound_alignment_example_sampler]\n",
    ")\n",
    "\n",
    "raw_train = (\n",
    "    raw_data[0][:8000], \n",
    "    raw_data[1][:8000], \n",
    "    raw_data[2][:8000],\n",
    "    raw_data[3][:8000]\n",
    ")\n",
    "raw_eval = (\n",
    "    raw_data[0][8000:9000], \n",
    "    raw_data[1][8000:9000], \n",
    "    raw_data[2][8000:9000],\n",
    "    raw_data[3][8000:9000]\n",
    ")\n",
    "raw_test = (\n",
    "    raw_data[0][9000:], \n",
    "    raw_data[1][9000:], \n",
    "    raw_data[2][9000:],\n",
    "    raw_data[3][9000:]\n",
    ")\n",
    "train_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_train[0], \n",
    "        \"source_input_ids\": raw_train[1],\n",
    "        \"labels\": raw_train[2],\n",
    "        \"intervention_ids\": raw_train[3],\n",
    "    }\n",
    ").with_format(\"torch\")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=16,\n",
    ")\n",
    "eval_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_eval[0], \n",
    "        \"source_input_ids\": raw_eval[1],\n",
    "        \"labels\": raw_eval[2],\n",
    "        \"intervention_ids\": raw_eval[3],\n",
    "    }\n",
    ").with_format(\"torch\")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=16,\n",
    ")\n",
    "test_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_test[0], \n",
    "        \"source_input_ids\": raw_test[1],\n",
    "        \"labels\": raw_test[2],\n",
    "        \"intervention_ids\": raw_test[3],\n",
    "    }\n",
    ").with_format(\"torch\")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f1eb2",
   "metadata": {},
   "source": [
    "First important check: since we randomly sample training data, although very unlikely due to the probability constraints from random combination of three numbers with 2 decimal points, we can check how many unique inputs we have during training, testing and eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a92bd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_train = set([tuple(e) for e in raw_train[0]])\n",
    "len(unique_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed637e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_all = set([tuple(e) for e in raw_train[0]] + [tuple(e) for e in raw_eval[0]] + [tuple(e) for e in raw_test[0]])\n",
    "len(unique_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba316528",
   "metadata": {},
   "source": [
    "Now, we are ready to align with this very simple dataset! Let's load back our model but with the alignable model architecture created by us.\n",
    "\n",
    "**Very Important**: If you want to load the model as `torch.bfloat16`, you are required to **replace** two torch files under your torch folder of the current running conda environment.\n",
    "\n",
    "To do this, a very hacky way is to first run the following script, and look at the place where the error is thrown. That is the place of the files you want to replace. With this, you can modify the following two bash command to replace your files. We have not fully tested this, but it seems like our replacing files are also compatible with torch 3.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d02f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# DO NOT RUN UNLESS YOU READ LINES ABOVE\n",
    "!cp ./torch3.8_overwrite/init.py /opt/conda/envs/BoundlessDAS/lib/python3.9/site-packages/torch/nn/\n",
    "!cp ./torch3.8_overwrite/parametrizations.py /opt/conda/envs/BoundlessDAS/lib/python3.9/site-packages/torch/nn/utils/\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748b87aa",
   "metadata": {},
   "source": [
    "Load the alignable model and others; then, create the aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915db602",
   "metadata": {},
   "outputs": [],
   "source": [
    "das_config = AlignableLlamaConfig.from_pretrained(\n",
    "    os.path.join(\"../alpaca_7b/\", \"das_config\")\n",
    ")\n",
    "alignment_config = {\n",
    "    'layer': das_config.das_layer,\n",
    "    \"token_range\" : [\n",
    "        das_config.das_token_range[0], \n",
    "        das_config.das_token_range[1], \n",
    "    ]\n",
    "}\n",
    "model = AutoAlignableModel.from_pretrained(\n",
    "    \"../alpaca_7b/\",\n",
    "    alignment_config=alignment_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "_ = model.to(\"cuda\") # first GPU\n",
    "\n",
    "# set off the gradients among all other layers.\n",
    "for name, param in model.named_parameters():\n",
    "    if \"rotate_layer\" not in name and \"intervention_boundaries\" not in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        logger.info(f\"Requiring gradients on layer: {name}\")\n",
    "t_total = int(len(train_dataloader) * 3)\n",
    "warm_up_steps = 0.1 * t_total\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params': model.model.rotate_layer.parameters()},\n",
    "    {'params': model.model.intervention_boundaries, 'lr': 1e-2}],\n",
    "    lr=1e-3\n",
    ")\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warm_up_steps,\n",
    "    num_training_steps=t_total\n",
    ")\n",
    "# You can define your custom compute_metrics function.\n",
    "def compute_metrics(eval_preds, eval_labels):\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    for eval_pred, eval_label in zip(eval_preds, eval_labels):\n",
    "        actual_test_labels = eval_label[:, -1]\n",
    "        pred_test_labels = torch.argmax(eval_pred[:, -1], dim=-1)\n",
    "        correct_labels = (actual_test_labels==pred_test_labels)\n",
    "        total_count += len(correct_labels)\n",
    "        correct_count += correct_labels.sum().tolist()\n",
    "    accuracy = round(correct_count/total_count, 2)\n",
    "    return {\"accuracy\" : accuracy}\n",
    "\n",
    "model_type = AutoConfig.from_pretrained(\n",
    "    \"../alpaca_7b/\"\n",
    ").architectures[0]\n",
    "\n",
    "run_name = f\"{model_type}.task.pricing_tag_lb.\"\\\n",
    "           f\"seed.42.intl.{alignment_config['layer']}.intr.{alignment_config['token_range'][0]}.\"\\\n",
    "           f\"{alignment_config['token_range'][1]}\"\n",
    "if not os.path.exists(\"./results_tmp/\"):\n",
    "    os.mkdir(\"./results_tmp/\")\n",
    "os.environ[\"WANDB_PROJECT\"] = f\"Boundless-DAS\"\n",
    "output_dir = os.path.join(\"./results_tmp/\", run_name)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "aligner = Aligner(\n",
    "    model,\n",
    "    logger=logger,\n",
    "    is_wandb=False,\n",
    "    is_master=True,\n",
    "    n_gpu=1,\n",
    "    model_name=run_name,\n",
    "    device=\"cuda\",\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b29656",
   "metadata": {},
   "source": [
    "Train Boundless DAS\n",
    "\n",
    "**Note that this is a simplied setting of what we've used in the paper to make the tutorial easier to follow.** The expectation is that, after the training is finished, we want to find an alignment of lower bound check within the Alpaca model. \n",
    "\n",
    "**Note that it is okay if the loss is go below zero!** since we optimize for smaller intervention site which could go below zero since we optimize for smaller loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34d089af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [07:54<00:00,  1.05it/s, loss=1.46]\n",
      "Epoch: 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [08:26<00:00,  1.01s/it, loss=-.09]\n",
      "Epoch: 2: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [08:27<00:00,  1.01s/it, loss=-1.03]\n",
      "Epoch: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [24:48<00:00, 496.13s/it]\n",
      "Training is finished ...\n"
     ]
    }
   ],
   "source": [
    "aligner.train(\n",
    "    train_dataloader, \n",
    "    eval_dataloader, \n",
    "    test_dataloader,\n",
    "    optimizer, scheduler, \n",
    "    log_step=10, \n",
    "    valid_steps=500,\n",
    "    output_dir=output_dir, \n",
    "    epochs=3, \n",
    "    gradient_accumulation_steps=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f0367c",
   "metadata": {},
   "source": [
    "#### Step 3: Analyzing Results\n",
    "\n",
    "We load the best rotation matrix saved in disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cec085d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_state_dict = torch.load(\n",
    "    f\"./results_tmp/LlamaForCausalLM.task.pricing_tag_lb.seed.42.intl.15.intr.80.81/pytorch-rotate-best.bin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790e05d",
   "metadata": {},
   "source": [
    "Let's look at what does this file contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71f3bfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rotate_layer': OrderedDict([('parametrizations.weight.original',\n",
       "               tensor([[ 0.0070, -0.0118,  0.0208,  ...,  0.0044,  0.0110,  0.0047],\n",
       "                       [-0.0074, -0.0139, -0.0033,  ...,  0.0156,  0.0061, -0.0041],\n",
       "                       [-0.0008,  0.0042,  0.0070,  ...,  0.0238,  0.0288, -0.0210],\n",
       "                       ...,\n",
       "                       [-0.0130, -0.0192, -0.0074,  ..., -0.0121,  0.0091, -0.0216],\n",
       "                       [ 0.0337, -0.0045,  0.0143,  ...,  0.0008, -0.0273,  0.0261],\n",
       "                       [-0.0168,  0.0146, -0.0071,  ...,  0.0160,  0.0156, -0.0077]],\n",
       "                      device='cuda:0', dtype=torch.bfloat16))]),\n",
       " 'intervention_boundaries': Parameter containing:\n",
       " tensor([ 0.1177, -0.6562], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " 'temperature': Parameter containing:\n",
       " tensor(16.8750, dtype=torch.bfloat16)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a4c51",
   "metadata": {},
   "source": [
    "Besides the rotation matrix, you can also see the intervention boundaries (in percentage) which marks the best setting for intervention accuracy. It is worth to note that this is boundary with the best evaluation metrics, not the lowest it can go in terms of maintaining a fairly good evaluation metrics. There is a subtle distinct here :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "572e2cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_state_dict = torch.load(\n",
    "    f\"./results_tmp/LlamaForCausalLM.task.pricing_tag_lb.seed.42.intl.15.intr.80.81/pytorch-rotate-last.bin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e191e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rotate_layer': OrderedDict([('parametrizations.weight.original',\n",
       "               tensor([[ 0.0070, -0.0118,  0.0208,  ...,  0.0044,  0.0110,  0.0047],\n",
       "                       [-0.0149, -0.0139, -0.0033,  ...,  0.0156,  0.0061, -0.0041],\n",
       "                       [-0.0039,  0.0037,  0.0070,  ...,  0.0238,  0.0288, -0.0210],\n",
       "                       ...,\n",
       "                       [-0.0087, -0.0134,  0.0008,  ..., -0.0121,  0.0091, -0.0216],\n",
       "                       [ 0.0327, -0.0143,  0.0172,  ..., -0.0013, -0.0273,  0.0261],\n",
       "                       [-0.0171,  0.0106, -0.0172,  ...,  0.0198,  0.0132, -0.0077]],\n",
       "                      device='cuda:0', dtype=torch.bfloat16))]),\n",
       " 'intervention_boundaries': Parameter containing:\n",
       " tensor([ 0.1069, -1.6328], device='cuda:0', dtype=torch.bfloat16,\n",
       "        requires_grad=True),\n",
       " 'temperature': Parameter containing:\n",
       " tensor(0.2002, dtype=torch.bfloat16)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520f4b6",
   "metadata": {},
   "source": [
    "The last rotation matrix is with slightly smaller site, but the temperature is much lower. As a result, the learned boundary is very sharp here.\n",
    "\n",
    "**Note that in the current paper, we report the best rotation matrix not the last one!** This creates an ambiguity in terms of intervention since it means the intervention can be **soft**. But the **Softness** can give nice properties, which is left to you to think about!\n",
    "\n",
    "Let's visualize both boundaries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acd0e1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid_boundary_sigmoid(_input, boundary_x, boundary_y, temperature):\n",
    "    return torch.sigmoid((_input - boundary_x) / temperature) * \\\n",
    "        torch.sigmoid((boundary_y - _input) / temperature)\n",
    "sns.set(rc={'figure.figsize':(5, 3)})\n",
    "sns.lineplot(\n",
    "    x=torch.arange(0, 4096), \n",
    "    y=sigmoid_boundary_sigmoid(torch.arange(0, 4096), 0, 4096*0.1177, 16.8750),\n",
    "    label='best'\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=torch.arange(0, 4096), \n",
    "    y=sigmoid_boundary_sigmoid(torch.arange(0, 4096), 0, 4096*0.1069, 0.2002),\n",
    "    label='last'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae62da",
   "metadata": {},
   "source": [
    "The above is the learned boundary! **Essentially, this is a low pass filter on the hidden representation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f7588",
   "metadata": {},
   "source": [
    "Now, with this rotation matrix, and learned intervention boundary, we can finally start to **steer** LLM's behavior following our aligned causal variables - the lower bound check!\n",
    "\n",
    "Instead of loading the best rotation matrix, we are using the last one to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6af524bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_input = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Please say yes only if it costs between 3.49 and 8.49 dollars, otherwise no.\n",
    "\n",
    "### Input:\n",
    "1.50 dollars\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e723ae",
   "metadata": {},
   "source": [
    "The output of this input should be **No** since **1.50 <= 3.49**. Let's check Alpaca's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08b3271c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afc label = No (20.75/21.125) ; pred label = No\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(base_input, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "attention_mask = tokenizer(base_input, return_tensors=\"pt\").attention_mask.to(\"cuda\")\n",
    "model.eval()\n",
    "outputs = model(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask\n",
    ")\n",
    "pred_labels = torch.argmax(outputs.logits[:, -1], dim=-1)\n",
    "generated_tokens = tokenizer.decode(pred_labels[0])\n",
    "\n",
    "afc_1 = tokenizer.convert_tokens_to_ids(\"Yes\")\n",
    "afc_2 = tokenizer.convert_tokens_to_ids(\"No\")\n",
    "afc_1_prob = outputs.logits[:, -1][0][afc_1]\n",
    "afc_2_prob = outputs.logits[:, -1][0][afc_2]\n",
    "if afc_1_prob > afc_2_prob:\n",
    "    afc = \"Yes\"\n",
    "else:\n",
    "    afc = \"No\"\n",
    "print(f\"afc label = {afc} ({afc_1_prob}/{afc_2_prob}) ; pred label = {generated_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647e57e",
   "metadata": {},
   "source": [
    "Now, let's intervene it to let the model to output **Yes** with our learned rotation matrix. \n",
    "\n",
    "The issue with **1.50** is that it is below the lower bound. So, we simply need another example that is above the lower bound. Let's take this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b22d1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_input = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Please say yes only if it costs between 2.51 and 7.51 dollars, otherwise no.\n",
    "\n",
    "### Input:\n",
    "9.49 dollars\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "input_ids = tokenizer(source_input, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "attention_mask = tokenizer(source_input, return_tensors=\"pt\").attention_mask.to(\"cuda\")\n",
    "model.eval()\n",
    "outputs = model(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    ")\n",
    "pred_labels = torch.argmax(outputs.logits[:, -1], dim=-1)\n",
    "generated_tokens = tokenizer.decode(pred_labels[0])\n",
    "\n",
    "afc_1 = tokenizer.convert_tokens_to_ids(\"Yes\")\n",
    "afc_2 = tokenizer.convert_tokens_to_ids(\"No\")\n",
    "afc_1_prob = outputs.logits[:, -1][0][afc_1]\n",
    "afc_2_prob = outputs.logits[:, -1][0][afc_2]\n",
    "if afc_1_prob > afc_2_prob:\n",
    "    afc = \"Yes\"\n",
    "else:\n",
    "    afc = \"No\"\n",
    "print(f\"afc label = {afc} ({afc_1_prob}/{afc_2_prob}) ; pred label = {generated_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412c773",
   "metadata": {},
   "source": [
    "This should output **No** as well since **9.49** is above both bounds! Now, since we aligned the lower bound check, we should be able to intervene the first example (lower bound check = False) with hidden states from the second example (lower bound check = True) representing the lower bound check. **As a result, we should be able to flip the output label.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73754dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afc label = Yes (21.375/20.25) ; pred label = Yes\n"
     ]
    }
   ],
   "source": [
    "source_hidden_states = model(\n",
    "    input_ids=tokenizer(source_input, return_tensors=\"pt\").input_ids.to(\"cuda\"),\n",
    "    output_rotated_hidden_states_only=True\n",
    ").rotated_hidden_states\n",
    "outputs = model(\n",
    "    input_ids=tokenizer(base_input, return_tensors=\"pt\").input_ids.to(\"cuda\"),\n",
    "    source_hidden_states=source_hidden_states,\n",
    "    intervention_ids=torch.tensor([0]).to(\"cuda\"),\n",
    ")\n",
    "pred_labels = torch.argmax(outputs.logits[:, -1], dim=-1)\n",
    "generated_tokens = tokenizer.decode(pred_labels[0])\n",
    "\n",
    "afc_1 = tokenizer.convert_tokens_to_ids(\"Yes\")\n",
    "afc_2 = tokenizer.convert_tokens_to_ids(\"No\")\n",
    "afc_1_prob = outputs.logits[:, -1][0][afc_1]\n",
    "afc_2_prob = outputs.logits[:, -1][0][afc_2]\n",
    "if afc_1_prob > afc_2_prob:\n",
    "    afc = \"Yes\"\n",
    "else:\n",
    "    afc = \"No\"\n",
    "print(f\"afc label = {afc} ({afc_1_prob}/{afc_2_prob}) ; pred label = {generated_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e113a840",
   "metadata": {},
   "source": [
    "**Yay! We successfully flip the label!** Within our paper, we should more experiments and have robustness checks. You can refer there for additional information if you ever doubt the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfce70c9",
   "metadata": {},
   "source": [
    "#### Step 4: Sharing Rotation Matrix and Alignment Config\n",
    "\n",
    "A **very promising future** of representation-based alignment search is **federated model steering** and **community sharing**! Let me elabrate them a little further, and with a specific example (i.e., sharing the lower bound boolean alignment through huggingface).\n",
    "\n",
    "- **federated model steering:** we all know prompts can steer model behaviors. However, prompt steering is not federated (i.e., users can steer the model into a harmful mode through prompt hacking). With intervention and representation-based alignment, we can control model representations through the causal lens and steer the model to behave in certain ways (e.g., anchoring the lower bound check to be always returing TRUE).\n",
    "\n",
    "- **community sharing:** the rotation matrix and learned boundary learned with boundless DAS can be shared through huggingface easily. This means, given a LLM, the community can do alignment and share meaningful alignment simply with the alignment config and learned roation matrix! This is just like sharing the prompt. Imagine a world where people find the alignment for gender. Then, we can do federated model steering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd2cd7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from huggingface_hub import HfApi\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2348b2",
   "metadata": {},
   "source": [
    "sharing the rotation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eccf61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"./results_tmp/LlamaForCausalLM.task.pricing_tag_lb.seed.42.intl.15.intr.80.81/pytorch-rotate-best.bin\",\n",
    "    path_in_repo=\"pytorch-rotate-best.bin\",\n",
    "    repo_id=\"zhengxuanzenwu/alpaca-price-tagging-lower-bound\",\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6048e0",
   "metadata": {},
   "source": [
    "sharing the alignment config (layer and token position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9ed023",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj=\"../alpaca_7b/das_config/config.json\",\n",
    "    path_in_repo=\"config.json\",\n",
    "    repo_id=\"zhengxuanzenwu/alpaca-price-tagging-lower-bound\",\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b9c16",
   "metadata": {},
   "source": [
    "You can find the rotation matrix and alignment config for this file here:   \n",
    "https://huggingface.co/zhengxuanzenwu/alpaca-price-tagging-lower-bound/tree/main\n",
    "\n",
    "You can download using huggingface API by following examples here:   \n",
    "https://huggingface.co/docs/huggingface_hub/guides/download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb6e1e",
   "metadata": {},
   "source": [
    "**Thank you** for your attention! If you like this tutorial, feel free to build on it, and share it with others!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
