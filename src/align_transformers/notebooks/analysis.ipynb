{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ae1dcea",
   "metadata": {},
   "source": [
    "#### main experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d16c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "from statistics import mean, variance\n",
    "import scipy.stats\n",
    "\n",
    "def min_max_scaler(data, original_min, original_max, min_value, max_value):\n",
    "    original_range = original_max - original_min\n",
    "    desired_range = max_value - min_value\n",
    "\n",
    "    scaled_data = {}\n",
    "    for k, number in data.items():\n",
    "        # Standardize the number\n",
    "        standardized_number = (number - original_min) / original_range\n",
    "\n",
    "        # Scale the standardized number to the desired range\n",
    "        scaled_number = (standardized_number * desired_range) + min_value\n",
    "        scaled_data[k] = scaled_number\n",
    "        \n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c99761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading multiple seeds results\n",
    "def aggregate_dict(dicts, aggregate_func=max):\n",
    "    results = {}\n",
    "    for d in dicts:\n",
    "        for k, v in d.items():\n",
    "            if k not in results:\n",
    "                results[k] = [v]\n",
    "            else:\n",
    "                results[k].append(v)\n",
    "    for k, v in results.items():\n",
    "        results[k] = aggregate_func(v)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790b23b",
   "metadata": {},
   "source": [
    "**You can ignore this section** \n",
    "\n",
    "We did not end up standardizing anything, so please ignore this section. Results are not included in our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lower_bounds = [0.5079, 0.5032, 0.5018, 0.5032]\n",
    "baseline_upper_bounds = [0.8516, 0.8516, 0.8516, 0.8516]\n",
    "\n",
    "loaded_objects = {\n",
    "    'lower_bound_alignment': [],\n",
    "    'both_bound_alignment': [],\n",
    "    'midpoint_alignment': [],\n",
    "    'bracket_alignment': [],\n",
    "}\n",
    "for seed in [42, 66, 77]:\n",
    "    with open(f'../logs/eval_main_results_seed_{seed}.pkl', 'rb') as file:\n",
    "        # Load the pickled object from the file\n",
    "        loaded_object = pickle.load(file)\n",
    "        for k, v in loaded_object.items():\n",
    "            loaded_objects[k] += [loaded_object[k]]\n",
    "\n",
    "oracle_lb_data = min_max_scaler(aggregate_dict(loaded_objects['lower_bound_alignment']), 0.5079, 0.8516, 0, 1)\n",
    "oracle_lub_data = min_max_scaler(aggregate_dict(loaded_objects['both_bound_alignment']), 0.5032, 0.8516, 0, 1)\n",
    "control_midpoint_data = min_max_scaler(aggregate_dict(loaded_objects['midpoint_alignment']), 0.5018, 0.8516, 0, 1)\n",
    "control_bracket_data = min_max_scaler(aggregate_dict(loaded_objects['bracket_alignment']), 0.5032, 0.8516, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(oracle_lb_data.items()), columns=['coord', 'value'])\n",
    "# Split the coord column into separate columns\n",
    "df[['LLaMA Layer Idx', 'Token Position Idx']] = pd.DataFrame(df['coord'].tolist(), index=df.index)\n",
    "df['LLaMA Layer Idx'] = df['LLaMA Layer Idx'].max() - df['LLaMA Layer Idx']\n",
    "# Reshape the DataFrame to have coordinates as columns\n",
    "heatmap_oracle_lb_df = df.pivot(index='LLaMA Layer Idx', columns='Token Position Idx', values='value')\n",
    "\n",
    "df = pd.DataFrame(list(oracle_lub_data.items()), columns=['coord', 'value'])\n",
    "# Split the coord column into separate columns\n",
    "df[['LLaMA Layer Idx', 'Token Position Idx']] = pd.DataFrame(df['coord'].tolist(), index=df.index)\n",
    "df['LLaMA Layer Idx'] = df['LLaMA Layer Idx'].max() - df['LLaMA Layer Idx']\n",
    "# Reshape the DataFrame to have coordinates as columns\n",
    "heatmap_oracle_lub_df = df.pivot(index='LLaMA Layer Idx', columns='Token Position Idx', values='value')\n",
    "\n",
    "df = pd.DataFrame(list(control_midpoint_data.items()), columns=['coord', 'value'])\n",
    "# Split the coord column into separate columns\n",
    "df[['LLaMA Layer Idx', 'Token Position Idx']] = pd.DataFrame(df['coord'].tolist(), index=df.index)\n",
    "df['LLaMA Layer Idx'] = df['LLaMA Layer Idx'].max() - df['LLaMA Layer Idx']\n",
    "# Reshape the DataFrame to have coordinates as columns\n",
    "heatmap_control_midpoint_df = df.pivot(index='LLaMA Layer Idx', columns='Token Position Idx', values='value')\n",
    "\n",
    "df = pd.DataFrame(list(control_bracket_data.items()), columns=['coord', 'value'])\n",
    "# Split the coord column into separate columns\n",
    "df[['LLaMA Layer Idx', 'Token Position Idx']] = pd.DataFrame(df['coord'].tolist(), index=df.index)\n",
    "df['LLaMA Layer Idx'] = df['LLaMA Layer Idx'].max() - df['LLaMA Layer Idx']\n",
    "# Reshape the DataFrame to have coordinates as columns\n",
    "heatmap_control_bracket_df = df.pivot(index='LLaMA Layer Idx', columns='Token Position Idx', values='value')\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
    "font = {'family' : 'DejaVu Serif',\n",
    "        'size'   : 14}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(14, 9.9))\n",
    "sns.heatmap(\n",
    "    heatmap_oracle_lb_df, yticklabels=heatmap_oracle_lb_df.index[::-1],\n",
    "    annot=True, fmt='.2f', vmin=0.0, vmax=1.0, \n",
    "    cmap=\"rocket_r\", square=True, cbar=False, ax=ax[0, 0], xticklabels=[]\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    heatmap_oracle_lub_df, yticklabels=heatmap_oracle_lub_df.index[::-1],\n",
    "    annot=True, fmt='.2f', vmin=0.0, vmax=1.0, \n",
    "    cmap=\"rocket_r\", square=True, cbar=False, ax=ax[0, 1], xticklabels=[]\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    heatmap_control_midpoint_df, yticklabels=heatmap_control_midpoint_df.index[::-1],\n",
    "    annot=True, fmt='.2f', vmin=0.0, vmax=1.0, \n",
    "    cmap=\"rocket_r\", square=True, cbar=False, ax=ax[1, 0],\n",
    "    xticklabels=[\"'<0x0A>' (69)\", \"'X' (70)\", \"'.' (71)\", \"'X' (72)\", \"'X' (73)\", \n",
    "                 \"'_dollars' (74)\", \"'<0x0A>' (75)\", \n",
    "                 \"'<0x0A>' (76)\", \"'##' (77)\", \"'#' (78)\", \n",
    "                 \"'_Response' (79)\", \"':' (80)\", \"'<0x0A>' (81)\"]\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    heatmap_control_bracket_df, yticklabels=heatmap_control_bracket_df.index[::-1],\n",
    "    annot=True, fmt='.2f', vmin=0.0, vmax=1.0, \n",
    "    cmap=\"rocket_r\", square=True, cbar=False, ax=ax[1, 1],\n",
    "    xticklabels=[\"'<0x0A>' (69)\", \"'X' (70)\", \"'.' (71)\", \"'X' (72)\", \"'X' (73)\", \n",
    "                 \"'_dollars' (74)\", \"'<0x0A>' (75)\", \n",
    "                 \"'<0x0A>' (76)\", \"'##' (77)\", \"'#' (78)\", \n",
    "                 \"'_Response' (79)\", \"':' (80)\", \"'<0x0A>' (81)\"]\n",
    ")\n",
    "ax[0, 0].set(xlabel=None,\n",
    "       ylabel='Alpaca (7B) Layer Index',\n",
    "       title='Left Boundary')\n",
    "ax[0, 1].set(xlabel=None,\n",
    "       ylabel='Alpaca (7B) Layer Index',\n",
    "       title='Left and Right Boundary')\n",
    "ax[1, 0].set(xlabel='Token (Index)',\n",
    "       ylabel='Alpaca (7B) Layer Index',\n",
    "       title='Mid-point Distance')\n",
    "ax[1, 1].set(xlabel='Token (Index)',\n",
    "       ylabel='Alpaca (7B) Layer Index',\n",
    "       title='Bracket Identity')\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(\"./logs/main-heatmap-standard.svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7297805",
   "metadata": {},
   "source": [
    "#### All Other Results with a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b72506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading all the results per seed, aggregate over seeds, and prepare for the table.\n",
    "main_results = {\n",
    "    'lower_bound_alignment': [],\n",
    "    'both_bound_alignment': [],\n",
    "    'midpoint_alignment': [],\n",
    "    'bracket_alignment': [],\n",
    "    'correct_results': [],\n",
    "    'incorrect_results': [],\n",
    "    'original': [],\n",
    "    'transfer': [],\n",
    "    'Context 1': [],\n",
    "    'Context 2': [],\n",
    "    'Sibling': [],\n",
    "}\n",
    "main_results_flatten = {\n",
    "    'lower_bound_alignment': [],\n",
    "    'both_bound_alignment': [],\n",
    "    'midpoint_alignment': [],\n",
    "    'bracket_alignment': [],\n",
    "    'correct_results': [],\n",
    "    'incorrect_results': [],\n",
    "    'original': [],\n",
    "    'transfer': [],\n",
    "    'Context 1': [],\n",
    "    'Context 2': [],\n",
    "    'Sibling': [],\n",
    "}\n",
    "for seed in [42, 66, 77]:\n",
    "    with open(f'../logs/eval_main_results_seed_{seed}.pkl', 'rb') as file:\n",
    "        # Load the pickled object from the file\n",
    "        loaded_object = pickle.load(file)\n",
    "        for k, v in loaded_object.items():\n",
    "            main_results[k] += [loaded_object[k]]\n",
    "            \n",
    "    with open(f'../logs/eval_results_consistency_seed_{seed}.pkl', 'rb') as file:\n",
    "        # Load the pickled object from the file\n",
    "        loaded_object = pickle.load(file)\n",
    "        for k, v in loaded_object.items():\n",
    "            main_results[k] += [loaded_object[k]]\n",
    "    \n",
    "    with open(f'../logs/eval_zero_shot_results_seed_{seed}.pkl', 'rb') as file:\n",
    "        # Load the pickled object from the file\n",
    "        loaded_object = pickle.load(file)\n",
    "        for k, v in loaded_object.items():\n",
    "            main_results[k] += [loaded_object[k]]\n",
    "    \n",
    "    with open(f'../logs/eval_results_irrelevant_context_seed_{seed}.pkl', 'rb') as file:\n",
    "        # Load the pickled object from the file\n",
    "        loaded_object = pickle.load(file)\n",
    "        for k, v in loaded_object.items():\n",
    "            if k == \"Pricing tag game!\":\n",
    "                main_results['Context 1'] += [loaded_object[k]]\n",
    "            elif k == \"Fruitarian Frogs May Be Doing Flowers a Favor\":\n",
    "                main_results['Context 2'] += [loaded_object[k]]\n",
    "            else:\n",
    "                main_results[k] += [loaded_object[k]]\n",
    "\n",
    "    # Open the pickled file for binary reading\n",
    "    with open(f'../logs/eval_different_return_results_seed_{seed}.pkl', 'rb') as file:\n",
    "        # Load the pickled object from the file\n",
    "        loaded_object = pickle.load(file)  \n",
    "        main_results['Sibling'] += [loaded_object]\n",
    "    \n",
    "for k, v in main_results.items():\n",
    "    aggregate_result = aggregate_dict(v)\n",
    "    for token_idx in [69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81]:\n",
    "        for layer_idx in [0, 5, 10, 15, 20, 25, 30]:\n",
    "            if k == \"Context 1\":\n",
    "                main_results_flatten[k].append(aggregate_result[(layer_idx, token_idx+6)])\n",
    "            elif k == \"Context 2\":\n",
    "                main_results_flatten[k].append(aggregate_result[(layer_idx, token_idx+15)])\n",
    "            else:\n",
    "                main_results_flatten[k].append(aggregate_result[(layer_idx, token_idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c743e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = []\n",
    "sequence = [\n",
    "    'lower_bound_alignment', 'both_bound_alignment', \\\n",
    "    'midpoint_alignment', 'bracket_alignment', \\\n",
    "    'correct_results', 'incorrect_results', \\\n",
    "    'original', 'transfer', 'Context 1', 'Context 2', \\\n",
    "    'Sibling'\n",
    "]\n",
    "for exp in sequence:\n",
    "    max_iia = max(main_results_flatten[exp])\n",
    "    var = variance(main_results_flatten[exp])*100\n",
    "    if exp in [\n",
    "        'lower_bound_alignment', 'both_bound_alignment', \\\n",
    "        'midpoint_alignment', 'bracket_alignment'\n",
    "    ]:\n",
    "        corr = 1.0\n",
    "    elif exp in ['original', 'transfer']:\n",
    "        corr = scipy.stats.pearsonr(main_results_flatten[exp], main_results_flatten['lower_bound_alignment'])[0]\n",
    "    else:\n",
    "        corr = scipy.stats.pearsonr(main_results_flatten[exp], main_results_flatten['both_bound_alignment'])[0]\n",
    "    table_data += [[max_iia, corr, var]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091da3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(table_data, columns =[\n",
    "    'IIA_max', 'Correlation', 'Variance'\n",
    "]) \n",
    "print(df.round(2).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a corner case\n",
    "scipy.stats.pearsonr(main_results_flatten['Sibling'][:-3], main_results_flatten['both_bound_alignment'][:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab75b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance(main_results_flatten['Sibling'][:-3])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee1a7fd",
   "metadata": {},
   "source": [
    "raw main results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa975dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_objects = {\n",
    "    'lower_bound_alignment': [],\n",
    "    'both_bound_alignment': [],\n",
    "    'midpoint_alignment': [],\n",
    "    'bracket_alignment': [],\n",
    "}\n",
    "for seed in [42, 66, 77]:\n",
    "    with open(f'./logs/eval_main_results_seed_{seed}.pkl', 'rb') as file:\n",
    "        # Load the pickled object from the file\n",
    "        loaded_object = pickle.load(file)\n",
    "        for k, v in loaded_object.items():\n",
    "            loaded_objects[k] += [loaded_object[k]]\n",
    "\n",
    "oracle_lb_data = aggregate_dict(loaded_objects['lower_bound_alignment'])\n",
    "oracle_lub_data = aggregate_dict(loaded_objects['both_bound_alignment'])\n",
    "control_midpoint_data = aggregate_dict(loaded_objects['midpoint_alignment'])\n",
    "control_bracket_data = aggregate_dict(loaded_objects['bracket_alignment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60fb37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(oracle_lb_data.items()), columns=['coord', 'value'])\n",
    "# Split the coord column into separate columns\n",
    "df[['LLaMA Layer Idx', 'Token Position Idx']] = pd.DataFrame(df['coord'].tolist(), index=df.index)\n",
    "df['LLaMA Layer Idx'] = df['LLaMA Layer Idx'].max() - df['LLaMA Layer Idx']\n",
    "# Reshape the DataFrame to have coordinates as columns\n",
    "heatmap_oracle_lb_df = df.pivot(index='LLaMA Layer Idx', columns='Token Position Idx', values='value')\n",
    "\n",
    "df = pd.DataFrame(list(oracle_lub_data.items()), columns=['coord', 'value'])\n",
    "# Split the coord column into separate columns\n",
    "df[['LLaMA Layer Idx', 'Token Position Idx']] = pd.DataFrame(df['coord'].tolist(), index=df.index)\n",
    "df['LLaMA Layer Idx'] = df['LLaMA Layer Idx'].max() - df['LLaMA Layer Idx']\n",
    "# Reshape the DataFrame to have coordinates as columns\n",
    "heatmap_oracle_lub_df = df.pivot(index='LLaMA Layer Idx', columns='Token Position Idx', values='value')\n",
    "\n",
    "df = pd.DataFrame(list(control_midpoint_data.items()), columns=['coord', 'value'])\n",
    "# Split the coord column into separate columns\n",
    "df[['LLaMA Layer Idx', 'Token Position Idx']] = pd.DataFrame(df['coord'].tolist(), index=df.index)\n",
    "df['LLaMA Layer Idx'] = df['LLaMA Layer Idx'].max() - df['LLaMA Layer Idx']\n",
    "# Reshape the DataFrame to have coordinates as columns\n",
    "heatmap_control_midpoint_df = df.pivot(index='LLaMA Layer Idx', columns='Token Position Idx', values='value')\n",
    "\n",
    "df = pd.DataFrame(list(control_bracket_data.items()), columns=['coord', 'value'])\n",
    "# Split the coord column into separate columns\n",
    "df[['LLaMA Layer Idx', 'Token Position Idx']] = pd.DataFrame(df['coord'].tolist(), index=df.index)\n",
    "df['LLaMA Layer Idx'] = df['LLaMA Layer Idx'].max() - df['LLaMA Layer Idx']\n",
    "# Reshape the DataFrame to have coordinates as columns\n",
    "heatmap_control_bracket_df = df.pivot(index='LLaMA Layer Idx', columns='Token Position Idx', values='value')\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
    "font = {'family' : 'DejaVu Serif',\n",
    "        'size'   : 14}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(14, 9.9))\n",
    "sns.heatmap(\n",
    "    heatmap_oracle_lb_df, yticklabels=heatmap_oracle_lb_df.index[::-1],\n",
    "    annot=True, fmt='.2f', vmin=0.0, vmax=1.0, \n",
    "    cmap=\"rocket_r\", square=True, cbar=False, ax=ax[0, 0], xticklabels=[]\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    heatmap_oracle_lub_df, yticklabels=heatmap_oracle_lub_df.index[::-1],\n",
    "    annot=True, fmt='.2f', vmin=0.0, vmax=1.0, \n",
    "    cmap=\"rocket_r\", square=True, cbar=False, ax=ax[0, 1], xticklabels=[]\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    heatmap_control_midpoint_df, yticklabels=heatmap_control_midpoint_df.index[::-1],\n",
    "    annot=True, fmt='.2f', vmin=0.0, vmax=1.0, \n",
    "    cmap=\"rocket_r\", square=True, cbar=False, ax=ax[1, 0],\n",
    "    xticklabels=[\"'<0x0A>' (69)\", \"'X' (70)\", \"'.' (71)\", \"'X' (72)\", \"'X' (73)\", \n",
    "                 \"'_dollars' (74)\", \"'<0x0A>' (75)\", \n",
    "                 \"'<0x0A>' (76)\", \"'##' (77)\", \"'#' (78)\", \n",
    "                 \"'_Response' (79)\", \"':' (80)\", \"'<0x0A>' (81)\"]\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    heatmap_control_bracket_df, yticklabels=heatmap_control_bracket_df.index[::-1],\n",
    "    annot=True, fmt='.2f', vmin=0.0, vmax=1.0, \n",
    "    cmap=\"rocket_r\", square=True, cbar=False, ax=ax[1, 1],\n",
    "    xticklabels=[\"'<0x0A>' (69)\", \"'X' (70)\", \"'.' (71)\", \"'X' (72)\", \"'X' (73)\", \n",
    "                 \"'_dollars' (74)\", \"'<0x0A>' (75)\", \n",
    "                 \"'<0x0A>' (76)\", \"'##' (77)\", \"'#' (78)\", \n",
    "                 \"'_Response' (79)\", \"':' (80)\", \"'<0x0A>' (81)\"]\n",
    ")\n",
    "ax[0, 0].set(xlabel=None,\n",
    "       ylabel='Alpaca (7B) Layer Index',\n",
    "       title='Left Boundary')\n",
    "ax[0, 1].set(xlabel=None,\n",
    "       ylabel='Alpaca (7B) Layer Index',\n",
    "       title='Left and Right Boundary')\n",
    "ax[1, 0].set(xlabel='Token (Index)',\n",
    "       ylabel='Alpaca (7B) Layer Index',\n",
    "       title='Mid-point Distance')\n",
    "ax[1, 1].set(xlabel='Token (Index)',\n",
    "       ylabel='Alpaca (7B) Layer Index',\n",
    "       title='Bracket Identity')\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(\"./logs/main-heatmap.svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc6c3d",
   "metadata": {},
   "source": [
    "raw consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e94cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_objects = {\n",
    "    'correct_results': [],\n",
    "    'incorrect_results': [],\n",
    "}\n",
    "for seed in [42, 66, 77]:\n",
    "    with open(f'./logs/eval_results_consistency_seed_{seed}.pkl', 'rb') as file:\n",
    "        # Load the pickled object from the file\n",
    "        loaded_object = pickle.load(file)\n",
    "        for k, v in loaded_object.items():\n",
    "            loaded_objects[k] += [loaded_object[k]]\n",
    "\n",
    "consistency_correct_data = aggregate_dict(loaded_objects['correct_results'])\n",
    "consistency_incorrect_data = aggregate_dict(loaded_objects['incorrect_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0945082",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(consistency_correct_data.items()), columns=['coord', 'value'])\n",
    "# Split the coord column into separate columns\n",
    "df[['LLaMA Layer Idx', 'Token Position Idx']] = pd.DataFrame(df['coord'].tolist(), index=df.index)\n",
    "df['LLaMA Layer Idx'] = df['LLaMA Layer Idx'].max() - df['LLaMA Layer Idx']\n",
    "# Reshape the DataFrame to have coordinates as columns\n",
    "heatmap_consistency_correct_df = df.pivot(index='LLaMA Layer Idx', columns='Token Position Idx', values='value')\n",
    "\n",
    "df = pd.DataFrame(list(consistency_incorrect_data.items()), columns=['coord', 'value'])\n",
    "# Split the coord column into separate columns\n",
    "df[['LLaMA Layer Idx', 'Token Position Idx']] = pd.DataFrame(df['coord'].tolist(), index=df.index)\n",
    "df['LLaMA Layer Idx'] = df['LLaMA Layer Idx'].max() - df['LLaMA Layer Idx']\n",
    "# Reshape the DataFrame to have coordinates as columns\n",
    "heatmap_consistency_incorrect_df = df.pivot(index='LLaMA Layer Idx', columns='Token Position Idx', values='value')\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
    "font = {'family' : 'DejaVu Serif',\n",
    "        'size'   : 14}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5.5))\n",
    "sns.heatmap(\n",
    "    heatmap_consistency_correct_df, yticklabels=heatmap_consistency_correct_df.index[::-1],\n",
    "    annot=True, fmt='.2f', vmin=0.0, vmax=1.0, \n",
    "    cmap=\"rocket_r\", square=True, cbar=False, ax=ax[0],\n",
    "    xticklabels=[\"'<0x0A>' (69)\", \"'X' (70)\", \"'.' (71)\", \"'X' (72)\", \"'X' (73)\", \n",
    "                 \"'_dollars' (74)\", \"'<0x0A>' (75)\", \n",
    "                 \"'<0x0A>' (76)\", \"'##' (77)\", \"'#' (78)\", \n",
    "                 \"'_Response' (79)\", \"':' (80)\", \"'<0x0A>' (81)\"]\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    heatmap_consistency_incorrect_df, yticklabels=heatmap_consistency_incorrect_df.index[::-1],\n",
    "    annot=True, fmt='.2f', vmin=0.0, vmax=1.0, \n",
    "    cmap=\"rocket_r\", square=True, cbar=False, ax=ax[1],\n",
    "    xticklabels=[\"'<0x0A>' (69)\", \"'X' (70)\", \"'.' (71)\", \"'X' (72)\", \"'X' (73)\", \n",
    "                 \"'_dollars' (74)\", \"'<0x0A>' (75)\", \n",
    "                 \"'<0x0A>' (76)\", \"'##' (77)\", \"'#' (78)\", \n",
    "                 \"'_Response' (79)\", \"':' (80)\", \"'<0x0A>' (81)\"]\n",
    ")\n",
    "\n",
    "ax[0].set(xlabel='Token (Index)',\n",
    "       ylabel='Alpaca (7B) Layer Index',\n",
    "       title='Left and Right Boundary (Correct Only)')\n",
    "ax[1].set(xlabel='Token (Index)',\n",
    "       ylabel='Alpaca (7B) Layer Index',\n",
    "       title='Left and Right Boundary (Incorrect Only)')\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(\"./logs/calibration.svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e039926",
   "metadata": {},
   "source": [
    "#### Irrelevant Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3016835",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_objects = {\n",
    "    'Pricing tag game!': [],\n",
    "    'Fruitarian Frogs May Be Doing Flowers a Favor': [],\n",
    "}\n",
    "for seed in [42, 66, 77]:\n",
    "    with open(f'./logs/eval_results_irrelevant_context_seed_{seed}.pkl', 'rb') as file:\n",
    "        # Load the pickled object from the file\n",
    "        loaded_object = pickle.load(file)\n",
    "        for k, v in loaded_object.items():\n",
    "            loaded_objects[k] += [loaded_object[k]]\n",
    "irrelevant_context_1 = aggregate_dict(loaded_objects['Pricing tag game!'])\n",
    "irrelevant_context_2 = aggregate_dict(loaded_objects['Fruitarian Frogs May Be Doing Flowers a Favor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba10f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(irrelevant_context_1.items()), columns=['coord', 'value'])\n",
    "# Split the coord column into separate columns\n",
    "df[['LLaMA Layer Idx', 'Token Position Idx']] = pd.DataFrame(df['coord'].tolist(), index=df.index)\n",
    "df['LLaMA Layer Idx'] = df['LLaMA Layer Idx'].max() - df['LLaMA Layer Idx']\n",
    "# Reshape the DataFrame to have coordinates as columns\n",
    "heatmap_context_1_df = df.pivot(index='LLaMA Layer Idx', columns='Token Position Idx', values='value')\n",
    "\n",
    "df = pd.DataFrame(list(irrelevant_context_2.items()), columns=['coord', 'value'])\n",
    "# Split the coord column into separate columns\n",
    "df[['LLaMA Layer Idx', 'Token Position Idx']] = pd.DataFrame(df['coord'].tolist(), index=df.index)\n",
    "df['LLaMA Layer Idx'] = df['LLaMA Layer Idx'].max() - df['LLaMA Layer Idx']\n",
    "# Reshape the DataFrame to have coordinates as columns\n",
    "heatmap_context_2_df = df.pivot(index='LLaMA Layer Idx', columns='Token Position Idx', values='value')\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
    "font = {'family' : 'DejaVu Serif',\n",
    "        'size'   : 14}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5.5))\n",
    "sns.heatmap(\n",
    "    heatmap_context_1_df, yticklabels=heatmap_context_1_df.index[::-1],\n",
    "    annot=True, fmt='.2f', vmin=0.0, vmax=1.0, \n",
    "    cmap=\"rocket_r\", square=True, cbar=False, ax=ax[0],\n",
    "    xticklabels=[\"'<0x0A>' (75)\", \"'X' (76)\", \"'.' (77)\", \"'X' (78)\", \"'X' (79)\", \n",
    "                 \"'_dollars' (80)\", \"'<0x0A>' (81)\", \n",
    "                 \"'<0x0A>' (82)\", \"'##' (83)\", \"'#' (84)\", \n",
    "                 \"'_Response' (85)\", \"':' (86)\", \"'<0x0A>' (87)\"]\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    heatmap_context_2_df, yticklabels=heatmap_context_2_df.index[::-1],\n",
    "    annot=True, fmt='.2f', vmin=0.0, vmax=1.0, \n",
    "    cmap=\"rocket_r\", square=True, cbar=False, ax=ax[1],\n",
    "    xticklabels=[\"'<0x0A>' (84)\", \"'X' (85)\", \"'.' (86)\", \"'X' (87)\", \"'X' (88)\", \n",
    "                 \"'_dollars' (89)\", \"'<0x0A>' (90)\", \n",
    "                 \"'<0x0A>' (91)\", \"'##' (92)\", \"'#' (93)\", \n",
    "                 \"'_Response' (94)\", \"':' (95)\", \"'<0x0A>' (96)\"]\n",
    ")\n",
    "\n",
    "ax[0].set(xlabel='Token (Index)',\n",
    "       ylabel='Alpaca (7B) Layer Index',\n",
    "       title=\"Prefix Context = 'Pricing tag game!'\")\n",
    "ax[1].set(xlabel='Token (Index)',\n",
    "       ylabel='Alpaca (7B) Layer Index',\n",
    "       title=\"Prefix Context = 'Fruitarian Frogs May Be \\nDoing Flowers a Favor'\")\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(\"./logs/irrelevant-context.svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5ffc5",
   "metadata": {},
   "source": [
    "#### Zero-shot between two tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d661fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_objects = {\n",
    "    'original': [],\n",
    "    'transfer': [],\n",
    "}\n",
    "for seed in [42, 66, 77]:\n",
    "    with open(f'./logs/eval_zero_shot_results_seed_{seed}.pkl', 'rb') as file:\n",
    "        # Load the pickled object from the file\n",
    "        loaded_object = pickle.load(file)\n",
    "        for k, v in loaded_object.items():\n",
    "            loaded_objects[k] += [loaded_object[k]]\n",
    "original = aggregate_dict(loaded_objects['original'])\n",
    "transfer = aggregate_dict(loaded_objects['transfer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e441872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(original.items()), columns=['coord', 'value'])\n",
    "# Split the coord column into separate columns\n",
    "df[['LLaMA Layer Idx', 'Token Position Idx']] = pd.DataFrame(df['coord'].tolist(), index=df.index)\n",
    "df['LLaMA Layer Idx'] = df['LLaMA Layer Idx'].max() - df['LLaMA Layer Idx']\n",
    "# Reshape the DataFrame to have coordinates as columns\n",
    "heatmap_original_df = df.pivot(index='LLaMA Layer Idx', columns='Token Position Idx', values='value')\n",
    "\n",
    "df = pd.DataFrame(list(transfer.items()), columns=['coord', 'value'])\n",
    "# Split the coord column into separate columns\n",
    "df[['LLaMA Layer Idx', 'Token Position Idx']] = pd.DataFrame(df['coord'].tolist(), index=df.index)\n",
    "df['LLaMA Layer Idx'] = df['LLaMA Layer Idx'].max() - df['LLaMA Layer Idx']\n",
    "# Reshape the DataFrame to have coordinates as columns\n",
    "heatmap_transfer_df = df.pivot(index='LLaMA Layer Idx', columns='Token Position Idx', values='value')\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
    "font = {'family' : 'DejaVu Serif',\n",
    "        'size'   : 14}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5.5))\n",
    "sns.heatmap(\n",
    "    heatmap_original_df, yticklabels=heatmap_original_df.index[::-1],\n",
    "    annot=True, fmt='.2f', vmin=0.0, vmax=1.0, \n",
    "    cmap=\"rocket_r\", square=True, cbar=False, ax=ax[0],\n",
    "    xticklabels=[\"'<0x0A>' (69)\", \"'X' (70)\", \"'.' (71)\", \"'X' (72)\", \"'X' (73)\", \n",
    "                 \"'_dollars' (74)\", \"'<0x0A>' (75)\", \n",
    "                 \"'<0x0A>' (76)\", \"'##' (77)\", \"'#' (78)\", \n",
    "                 \"'_Response' (79)\", \"':' (80)\", \"'<0x0A>' (81)\"]\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    heatmap_transfer_df, yticklabels=heatmap_transfer_df.index[::-1],\n",
    "    annot=True, fmt='.2f', vmin=0.0, vmax=1.0, \n",
    "    cmap=\"rocket_r\", square=True, cbar=False, ax=ax[1],\n",
    "    xticklabels=[\"'<0x0A>' (69)\", \"'X' (70)\", \"'.' (71)\", \"'X' (72)\", \"'X' (73)\", \n",
    "                 \"'_dollars' (74)\", \"'<0x0A>' (75)\", \n",
    "                 \"'<0x0A>' (76)\", \"'##' (77)\", \"'#' (78)\", \n",
    "                 \"'_Response' (79)\", \"':' (80)\", \"'<0x0A>' (81)\"]\n",
    ")\n",
    "\n",
    "ax[0].set(xlabel='Token (Index)',\n",
    "       ylabel='Alpaca (7B) Layer Index',\n",
    "       title=\"'between 5.49 and 8.49 dollars' (Seen)\")\n",
    "ax[1].set(xlabel='Token (Index)',\n",
    "       ylabel='Alpaca (7B) Layer Index',\n",
    "       title=\"'between 2.51 and 5.51 dollars' (Unseen)\")\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(\"./logs/zeroshot-transfer.svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c0acc8",
   "metadata": {},
   "source": [
    "#### Zero-shot sibiling instruction transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25665310",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_objects = {\n",
    "    'original': [],\n",
    "    'transfer': [],\n",
    "}\n",
    "for seed in [42, 66, 77]:\n",
    "    with open(f'./logs/eval_main_results_seed_{seed}.pkl', 'rb') as file:\n",
    "        # Load the pickled object from the file\n",
    "        loaded_object = pickle.load(file)\n",
    "        loaded_objects['original'] += [loaded_object['both_bound_alignment']]\n",
    "        \n",
    "    with open(f'./logs/eval_different_return_results_seed_{seed}.pkl', 'rb') as file:\n",
    "        # Load the pickled object from the file\n",
    "        loaded_object = pickle.load(file)\n",
    "        loaded_objects['transfer'] += [loaded_object]\n",
    "original = aggregate_dict(loaded_objects['original'])\n",
    "transfer = aggregate_dict(loaded_objects['transfer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef21243",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(original.items()), columns=['coord', 'value'])\n",
    "# Split the coord column into separate columns\n",
    "df[['LLaMA Layer Idx', 'Token Position Idx']] = pd.DataFrame(df['coord'].tolist(), index=df.index)\n",
    "df['LLaMA Layer Idx'] = df['LLaMA Layer Idx'].max() - df['LLaMA Layer Idx']\n",
    "# Reshape the DataFrame to have coordinates as columns\n",
    "heatmap_original_df = df.pivot(index='LLaMA Layer Idx', columns='Token Position Idx', values='value')\n",
    "\n",
    "df = pd.DataFrame(list(transfer.items()), columns=['coord', 'value'])\n",
    "# Split the coord column into separate columns\n",
    "df[['LLaMA Layer Idx', 'Token Position Idx']] = pd.DataFrame(df['coord'].tolist(), index=df.index)\n",
    "df['LLaMA Layer Idx'] = df['LLaMA Layer Idx'].max() - df['LLaMA Layer Idx']\n",
    "# Reshape the DataFrame to have coordinates as columns\n",
    "heatmap_transfer_df = df.pivot(index='LLaMA Layer Idx', columns='Token Position Idx', values='value')\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
    "font = {'family' : 'DejaVu Serif',\n",
    "        'size'   : 14}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5.5))\n",
    "sns.heatmap(\n",
    "    heatmap_original_df, yticklabels=heatmap_original_df.index[::-1],\n",
    "    annot=True, fmt='.2f', vmin=0.0, vmax=1.0, \n",
    "    cmap=\"rocket_r\", square=True, cbar=False, ax=ax[0],\n",
    "    xticklabels=[\"'<0x0A>' (69)\", \"'X' (70)\", \"'.' (71)\", \"'X' (72)\", \"'X' (73)\", \n",
    "                 \"'_dollars' (74)\", \"'<0x0A>' (75)\", \n",
    "                 \"'<0x0A>' (76)\", \"'##' (77)\", \"'#' (78)\", \n",
    "                 \"'_Response' (79)\", \"':' (80)\", \"'<0x0A>' (81)\"]\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    heatmap_transfer_df, yticklabels=heatmap_transfer_df.index[::-1],\n",
    "    annot=True, fmt='.2f', vmin=0.0, vmax=1.0, \n",
    "    cmap=\"rocket_r\", square=True, cbar=False, ax=ax[1],\n",
    "    xticklabels=[\"'<0x0A>' (69)\", \"'X' (70)\", \"'.' (71)\", \"'X' (72)\", \"'X' (73)\", \n",
    "                 \"'_dollars' (74)\", \"'<0x0A>' (75)\", \n",
    "                 \"'<0x0A>' (76)\", \"'##' (77)\", \"'#' (78)\", \n",
    "                 \"'_Response' (79)\", \"':' (80)\", \"'<0x0A>' (81)\"]\n",
    ")\n",
    "\n",
    "ax[0].set(xlabel='Token (Index)',\n",
    "       ylabel='Alpaca (7B) Layer Index',\n",
    "       title=\"'Say yes or no' (Seen)\")\n",
    "ax[1].set(xlabel='Token (Index)',\n",
    "       ylabel='Alpaca (7B) Layer Index',\n",
    "       title=\"'Say True or False' (Unseen)\")\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(\"./logs/different-return-transfer.svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86adaf2b",
   "metadata": {},
   "source": [
    "#### plot the boundary learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be47c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wandb_csv(raw_df):\n",
    "    data = {}\n",
    "    raw_df = raw_df.fillna(0)\n",
    "    for column_name, column_data in raw_df.iteritems():\n",
    "        if \"alpaca\" in column_name and \"MAX\" not in column_name and \"MIN\" not in column_name:\n",
    "            layer = int(column_name.split(\".\")[-4])\n",
    "            token_id = int(column_name.split(\".\")[-2])\n",
    "            data[(layer, token_id)] = column_data.tolist()\n",
    "    return data\n",
    "\n",
    "def slice_data(data, flag):\n",
    "    positive_data = []\n",
    "    negative_data = []\n",
    "    for k, v in data.items():\n",
    "        if flag[k]:\n",
    "            positive_data += [v]\n",
    "        else:\n",
    "            negative_data += [v]\n",
    "    return positive_data, negative_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56de962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
    "font = {'family' : 'DejaVu Serif',\n",
    "        'size'   : 14}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# i am lazy ...\n",
    "eval_acc_file = \"./logs/lb_eval_acc.csv\"\n",
    "boundary_file = \"./logs/lb_boundary.csv\"\n",
    "train_acc_file = \"./logs/lb_train_acc.csv\"\n",
    "eval_acc_df = pd.read_csv(eval_acc_file)\n",
    "boundary_df = pd.read_csv(boundary_file)\n",
    "train_acc_df = pd.read_csv(train_acc_file)\n",
    "eval_acc_data = process_wandb_csv(eval_acc_df)\n",
    "boundary_data = process_wandb_csv(boundary_df)\n",
    "train_acc_data = process_wandb_csv(train_acc_df)\n",
    "boundary_boolean_data = {}\n",
    "for k, v in boundary_data.items():\n",
    "    if v[-1] < 1e-2:\n",
    "        boundary_boolean_data[k] = False\n",
    "    else:\n",
    "        boundary_boolean_data[k] = True\n",
    "eval_acc_positive, eval_acc_negative = slice_data(eval_acc_data, boundary_boolean_data)\n",
    "train_acc_positive, train_acc_negative = slice_data(train_acc_data, boundary_boolean_data)\n",
    "boundary_positive, boundary_negative = slice_data(boundary_data, boundary_boolean_data)\n",
    "\n",
    "# Convert the data to numpy arrays\n",
    "data_array1 = np.array(eval_acc_positive)\n",
    "data_array2 = np.array(eval_acc_negative)\n",
    "\n",
    "# Calculate the mean and standard deviation along the time axis (axis=0)\n",
    "mean1 = np.mean(data_array1, axis=0)\n",
    "std_dev1 = np.std(data_array1, axis=0)\n",
    "\n",
    "mean2 = np.mean(data_array2, axis=0)\n",
    "std_dev2 = np.std(data_array2, axis=0)\n",
    "\n",
    "# it is unfortunate that i did not include the starting point data,\n",
    "# i have to include this buy hand here.\n",
    "mean1 = np.append([0.49], mean1, axis=0)\n",
    "std_dev1 = np.append([0.02], std_dev1, axis=0)\n",
    "mean2 = np.append([0.49], mean2, axis=0)\n",
    "std_dev2 = np.append([0.02], std_dev2, axis=0)\n",
    "\n",
    "# Create the x-axis as a range of time steps\n",
    "acc_steps = [i*500 for i in range(0, len(mean1))]\n",
    "\n",
    "# Plot the mean and standard deviation as shaded regions for both datasets\n",
    "ax[0].plot(\n",
    "    acc_steps, mean1, label='IIA (Aligned)',\n",
    "    marker='^', color='#2ca02c', markersize=8, linewidth=2,\n",
    "    markerfacecolor='none', markeredgecolor='#2ca02c', markeredgewidth=3\n",
    ")\n",
    "ax[0].fill_between(acc_steps, mean1 - std_dev1, mean1 + std_dev1, color='#2ca02c', alpha=0.1)\n",
    "\n",
    "ax[0].plot(\n",
    "    acc_steps, mean2, label='IIA (Unaligned)',\n",
    "    marker='o', color='#1f77b4', markersize=8, linewidth=2, \n",
    "    markerfacecolor='none', markeredgecolor='#1f77b4', markeredgewidth=3\n",
    ")\n",
    "ax[0].fill_between(acc_steps, mean2 - std_dev2, mean2 + std_dev2, color='#1f77b4', alpha=0.1)\n",
    "\n",
    "# Convert the data to numpy arrays\n",
    "data_array1 = np.array(boundary_positive)\n",
    "data_array2 = np.array(boundary_negative)\n",
    "\n",
    "# Calculate the mean and standard deviation along the time axis (axis=0)\n",
    "mean1 = np.mean(data_array1, axis=0)\n",
    "std_dev1 = np.std(data_array1, axis=0)\n",
    "\n",
    "mean2 = np.mean(data_array2, axis=0)\n",
    "std_dev2 = np.std(data_array2, axis=0)\n",
    "\n",
    "boundary_steps = [i*10 for i in range(1, len(mean1)+1)]\n",
    "\n",
    "# Plot the mean and standard deviation as shaded regions for both datasets\n",
    "ax[0].plot(\n",
    "    boundary_steps, mean1, label='Width (Aligned)', color='#2ca02c', linewidth=2, \n",
    ")\n",
    "ax[0].fill_between(boundary_steps, mean1 - std_dev1, mean1 + std_dev1, color='#2ca02c', alpha=0.2)\n",
    "\n",
    "ax[0].plot(\n",
    "    boundary_steps, mean2, label='Width (Unaligned)', color='#1f77b4', linewidth=2, \n",
    ")\n",
    "ax[0].fill_between(boundary_steps, mean2 - std_dev2, mean2 + std_dev2, color='#1f77b4', alpha=0.2)\n",
    "\n",
    "# i am lazy ...\n",
    "eval_acc_file = \"./logs/lub_eval_acc.csv\"\n",
    "boundary_file = \"./logs/lub_boundary.csv\"\n",
    "eval_acc_df = pd.read_csv(eval_acc_file)\n",
    "boundary_df = pd.read_csv(boundary_file)\n",
    "eval_acc_data = process_wandb_csv(eval_acc_df)\n",
    "boundary_data = process_wandb_csv(boundary_df)\n",
    "boundary_boolean_data = {}\n",
    "for k, v in boundary_data.items():\n",
    "    if v[-1] < 1e-2:\n",
    "        boundary_boolean_data[k] = False\n",
    "    else:\n",
    "        boundary_boolean_data[k] = True\n",
    "eval_acc_positive, eval_acc_negative = slice_data(eval_acc_data, boundary_boolean_data)\n",
    "boundary_positive, boundary_negative = slice_data(boundary_data, boundary_boolean_data)\n",
    "\n",
    "# Convert the data to numpy arrays\n",
    "data_array1 = np.array(eval_acc_positive)\n",
    "data_array2 = np.array(eval_acc_negative)\n",
    "\n",
    "# Calculate the mean and standard deviation along the time axis (axis=0)\n",
    "mean1 = np.mean(data_array1, axis=0)\n",
    "std_dev1 = np.std(data_array1, axis=0)\n",
    "\n",
    "mean2 = np.mean(data_array2, axis=0)\n",
    "std_dev2 = np.std(data_array2, axis=0)\n",
    "\n",
    "# it is unfortunate that i did not include the starting point data,\n",
    "# i have to include this buy hand here.\n",
    "mean1 = np.append([0.49], mean1, axis=0)\n",
    "std_dev1 = np.append([0.02], std_dev1, axis=0)\n",
    "mean2 = np.append([0.49], mean2, axis=0)\n",
    "std_dev2 = np.append([0.02], std_dev2, axis=0)\n",
    "\n",
    "# Create the x-axis as a range of time steps\n",
    "acc_steps = [i*500 for i in range(0, len(mean1))]\n",
    "\n",
    "# Plot the mean and standard deviation as shaded regions for both datasets\n",
    "ax[1].plot(\n",
    "    acc_steps, mean1, label='IIA (Aligned)',\n",
    "    marker='^', color='#2ca02c', markersize=8, linewidth=2,\n",
    "    markerfacecolor='none', markeredgecolor='#2ca02c', markeredgewidth=3\n",
    ")\n",
    "ax[1].fill_between(acc_steps, mean1 - std_dev1, mean1 + std_dev1, color='#2ca02c', alpha=0.1)\n",
    "\n",
    "ax[1].plot(\n",
    "    acc_steps, mean2, label='IIA (Unaligned)',\n",
    "    marker='o', color='#1f77b4', markersize=8, linewidth=2, \n",
    "    markerfacecolor='none', markeredgecolor='#1f77b4', markeredgewidth=3\n",
    ")\n",
    "ax[1].fill_between(acc_steps, mean2 - std_dev2, mean2 + std_dev2, color='#1f77b4', alpha=0.1)\n",
    "\n",
    "# Convert the data to numpy arrays\n",
    "data_array1 = np.array(boundary_positive)\n",
    "data_array2 = np.array(boundary_negative)\n",
    "\n",
    "# Calculate the mean and standard deviation along the time axis (axis=0)\n",
    "mean1 = np.mean(data_array1, axis=0)\n",
    "std_dev1 = np.std(data_array1, axis=0)\n",
    "\n",
    "mean2 = np.mean(data_array2, axis=0)\n",
    "std_dev2 = np.std(data_array2, axis=0)\n",
    "\n",
    "boundary_steps = [i*10 for i in range(1, len(mean1)+1)]\n",
    "\n",
    "# Plot the mean and standard deviation as shaded regions for both datasets\n",
    "ax[1].plot(\n",
    "    boundary_steps, mean1, label='Width (Aligned)', color='#2ca02c', linewidth=2, \n",
    ")\n",
    "ax[1].fill_between(boundary_steps, mean1 - std_dev1, mean1 + std_dev1, color='#2ca02c', alpha=0.2)\n",
    "\n",
    "ax[1].plot(\n",
    "    boundary_steps, mean2, label='Width (Unaligned)', color='#1f77b4', linewidth=2, \n",
    ")\n",
    "ax[1].fill_between(boundary_steps, mean2 - std_dev2, mean2 + std_dev2, color='#1f77b4', alpha=0.2)\n",
    "\n",
    "ax[0].set_xlabel('Training Steps')\n",
    "ax[0].set_ylabel('Value')\n",
    "ax[0].set_title('Boundary Checks (Left Only)')\n",
    "ax[0].legend(loc='upper right', fontsize=\"12\", ncol=2)\n",
    "ax[0].set_ylim(-0.1, 1.1)\n",
    "ax[0].set_xlim(0, 6700)\n",
    "\n",
    "ax[1].set_xlabel('Training Steps')\n",
    "ax[1].set_ylabel('Value')\n",
    "ax[1].set_title('Boundary Checks (Both)')\n",
    "ax[1].legend(loc='upper right', fontsize=\"12\", ncol=2)\n",
    "ax[1].set_ylim(-0.1, 1.1)\n",
    "ax[1].set_xlim(0, 6700)\n",
    "\n",
    "ax[0].grid(which='major', axis='x', linestyle='--', color='gray', alpha=0.3)\n",
    "ax[0].grid(which='major', axis='y', linestyle='--', color='gray', alpha=0.3)\n",
    "ax[1].grid(which='major', axis='y', linestyle='--', color='gray', alpha=0.3)\n",
    "ax[1].grid(which='major', axis='x', linestyle='--', color='gray', alpha=0.3)\n",
    "\n",
    "# Show the plot\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig(\"./logs/boundary-learning.svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaec52a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
