{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "685d6052",
   "metadata": {},
   "source": [
    "### Chapter 1: Dataset Introduction\n",
    "\n",
    "This section contains step-by-step instructions about how we create this simple logic game dataset. The final part of the output contains all the logic programs we selected to train the model. This script can be also used to generate other logic programs, and our set is not an exhaustive set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "359a15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "    \n",
    "set_seed(42)\n",
    "TOKEN_RUN_ALREADY = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a8a5b",
   "metadata": {},
   "source": [
    "#### Token identity, word identity and more\n",
    "\n",
    "To isolate the problem of token identity and word identity mapping from causal mechanism discovery, we support two versions of the dataset: 1) inputs are just GPT-2 tokens; 2) inputs are English words. The second option will eventually lead to a case where clear word to token identity mapping becomes crucial. We focus on the first case.\n",
    "\n",
    "To create the token vocab for our task, we need one type of tokens that some of them are considered as synonyms and antonyms to each other. We use the embedding distance metrics to control for that (i.e., for each token, we get the top 10 ngbr tokens, and mark them as the synonyms of this word; and the top 10 furthest ngbr tokens as the antonyms.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c196ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TOKEN_RUN_ALREADY:\n",
    "    \n",
    "    def only_letters(tested_string):\n",
    "        match = re.match(\"^([A-Z]|[a-z])*$\", tested_string)\n",
    "        return match is not None\n",
    "    \n",
    "    embeddings_gpt2 = GPT2LMHeadModel.from_pretrained(\n",
    "        \"gpt2\",\n",
    "        cache_dir=\"./.cache_dir/\"\n",
    "    ).transformer.wte.weight.data\n",
    "    embeddings_gpt2 = F.normalize(embeddings_gpt2, dim=1)\n",
    "    vocab_map = {}\n",
    "    for k, v in tokenizer.get_vocab().items():\n",
    "        vocab_map[v] = k\n",
    "\n",
    "    closest_nbrs_map = {}\n",
    "    for k, v in tqdm(tokenizer.get_vocab().items()):\n",
    "        if v < 256 or 'Ġ' in k or len(k) < 3 or not only_letters(k) or 'Ã' in k:\n",
    "            continue\n",
    "        v_embed = embeddings_gpt2[v]\n",
    "        cosine_sim = torch.mm(embeddings_gpt2, v_embed.unsqueeze(dim=-1))\n",
    "        values, indices = torch.topk(cosine_sim.squeeze(dim=-1), 100)\n",
    "\n",
    "        closest_nbrs = []\n",
    "        for i in range(1, len(indices)):\n",
    "            w = vocab_map[indices[i].tolist()]\n",
    "            if 'Ġ' not in w and len(w) > 2 and values[i] >=0.5 and 'Ã' not in w and only_letters(w):\n",
    "                closest_nbrs += [w]\n",
    "        if len(closest_nbrs) != 0:\n",
    "            closest_nbrs_map[k] = closest_nbrs    \n",
    "    pairs = set([])\n",
    "    for k, vs in closest_nbrs_map.items():\n",
    "        for v in vs:\n",
    "            if (k, v) not in pairs:\n",
    "                pairs.add((k, v))\n",
    "    file = open('token_cos_synonyms.txt', 'w')\n",
    "    for pair in pairs:\n",
    "        file.write(f\"{pair[0]} - {pair[1]}\\n\")\n",
    "    file.close()\n",
    "    \n",
    "    \"\"\"\n",
    "    This is to check whether we can have the same sequence length,\n",
    "    since this is the point of doing this token-based data generation.\n",
    "    \"\"\"\n",
    "    all_vocab, synonyms_pairs, synonyms_dict = fetch_metadata(\".\", use_token=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    for _ in range(10000):\n",
    "        seq = \",\".join(random.sample(all_vocab, k=5))\n",
    "        if len(tokenizer.tokenize(seq)) != 5+4:\n",
    "            print(seq, tokenizer.tokenize(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e846f115",
   "metadata": {},
   "source": [
    "#### Loading some pre-generated vocabs.\n",
    "\n",
    "Here, we focus on a set of synonyms and antonyms pairs generated by GPT-4. We use them as the vocab of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "408083c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab, synonyms_pairs, synonyms_dict = fetch_metadata(\".\", use_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5ddc49",
   "metadata": {},
   "source": [
    "#### Using our own DSLs, and a sort of PCFG, we generate all the programs that are possible.\n",
    "\n",
    "Note that these programs are not all valid! In fact, a lot of them can be equivalent programs which we need to filter out. The following steps include our first filtering step applied, which is to run the same input-output pairs for each program, and filter out those programs have biased output label distribution (e.g., 100% False and 0% True) just to make the results a little cleaner afterwards.\n",
    "\n",
    "**Worth for you to think**: by making the problem cleaner, we are also sort of *making the interpretability problem easier*. After a very restrict filtering, it almost can be seen as there are only certain ways to solve each program at the end, which could potentially make the circuit discovery process easier. But, as we focus on interpretability after all, easier-to-find circuits are still interesting and relevant. It is good to keep this mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "programs = generate_programs()\n",
    "balanced_programs = []\n",
    "program_value_map = {}\n",
    "for p in programs:\n",
    "    program_value_map[p] = []\n",
    "    for i in range(1000):\n",
    "        outputs = sample_factual_inputs(p, all_vocab, synonyms_pairs, synonyms_dict)\n",
    "        eval_out = eval_program(outputs[0], outputs[1], synonyms_pairs, synonyms_dict)\n",
    "        if eval_out != outputs[-1]['op5']:\n",
    "            print(outputs)\n",
    "            print(eval_out)\n",
    "            FAIL()\n",
    "        program_value_map[p].append(outputs[-1]['op5'])\n",
    "    counter = Counter(program_value_map[p])\n",
    "    if float(counter[True]/counter[False]) <= 1.5 and float(counter[True]/counter[False]) >= 0.66:\n",
    "        balanced_programs.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de0c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total n program = {len(programs)}; balanced n program = {len(balanced_programs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc4495b",
   "metadata": {},
   "source": [
    "#### Potentially equivalent program detection\n",
    "\n",
    "Given a set of inputs, we measure the correlation of outputs between two programs when evaluated on the same inputs. Obviously, a program will correlate with itself with a coefficient of 1.0. And the higher, the more correlated two programs are. We filter out programs that are very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aada97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baysian checks.\n",
    "progress_count = 0\n",
    "all_scores = []\n",
    "for p in tqdm(balanced_programs):\n",
    "    progress_count += 1\n",
    "    outputs_collect = []\n",
    "    p_outs = []\n",
    "    for i in range(1000):\n",
    "        outputs = sample_factual_inputs(p, all_vocab, synonyms_pairs, synonyms_dict)\n",
    "        outputs_collect += [outputs]\n",
    "        p_outs += [outputs[-1]['op5']]\n",
    "    p_outs = np.array(p_outs)\n",
    "    \n",
    "    similarity_scores = []\n",
    "    for compare_to_p in balanced_programs:\n",
    "        eval_outs = []\n",
    "        for outputs in outputs_collect:\n",
    "            eval_out = eval_program(compare_to_p, outputs[1], synonyms_pairs, synonyms_dict)\n",
    "            eval_outs += [eval_out]\n",
    "        similarity_score = (np.array(eval_outs)==p_outs).sum()/len(outputs_collect)\n",
    "        similarity_scores += [similarity_score]\n",
    "    all_scores += [similarity_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738f3704",
   "metadata": {},
   "source": [
    "Let's visualize the correlations between programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e720d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = np.array(all_scores)\n",
    "sns.heatmap(all_scores, square=True)\n",
    "plt.xlim(0, all_scores.shape[0])\n",
    "plt.ylim(0, all_scores.shape[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bbc022",
   "metadata": {},
   "source": [
    "Now, for each program, we can then have a \"good list\" of programs that are weakly correlate with it. Note that the score is represented by the count of same outputs between two programs. If the number is very low, meaning it is very easy to distinguish these two programs, which means we don't need to have that many demonstrations. So, we want to make the task harder to restrict the lower bound.\n",
    "\n",
    "We simply use a greedy-search to find that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91300c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_set = list(weakly_correlated_variables(all_scores, 0.7))\n",
    "filtered_balanced_programs = [balanced_programs[idx] for idx in list(current_set)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5ec2f0",
   "metadata": {},
   "source": [
    "We then verify this set is good, a.k.a there is no pair of programs have highly correlated input-output behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213cfed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_scores = []\n",
    "for p in tqdm(filtered_balanced_programs):\n",
    "    outputs_collect = []\n",
    "    p_outs = []\n",
    "    for i in range(1000):\n",
    "        outputs = sample_factual_inputs(p, all_vocab, synonyms_pairs, synonyms_dict)\n",
    "        outputs_collect += [outputs]\n",
    "        p_outs += [outputs[-1]['op5']]\n",
    "    p_outs = np.array(p_outs)\n",
    "    \n",
    "    similarity_scores = []\n",
    "    for compare_to_p in filtered_balanced_programs:\n",
    "        eval_outs = []\n",
    "        for outputs in outputs_collect:\n",
    "            eval_out = eval_program(compare_to_p, outputs[1], synonyms_pairs, synonyms_dict)\n",
    "            eval_outs += [eval_out]\n",
    "        similarity_score = (np.array(eval_outs)==p_outs).sum()/len(outputs_collect)\n",
    "        similarity_scores += [similarity_score]\n",
    "    filtered_scores += [similarity_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825a7aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_scores = np.array(filtered_scores)\n",
    "sns.heatmap(filtered_scores, square=True)\n",
    "plt.xlim(0, filtered_scores.shape[0])\n",
    "plt.ylim(0, filtered_scores.shape[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf33c40",
   "metadata": {},
   "source": [
    "#### DONE!\n",
    "\n",
    "Okay, now we have a good list of programs (**30!**) to start to build our meta-learner, and do alignments on the meta-learner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47627ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = open(\"seed_programs.json\", \"w\")\n",
    "json.dump(filtered_balanced_programs, out_file, indent = 2)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d1e768",
   "metadata": {},
   "source": [
    "### Chapter 2: Dataset Creation\n",
    "\n",
    "We will demo functions to create factual training data, as well as counterfactual training data for alignment. Note that, since the task is a simple synthetic task, the actual dataset will be created on-the-fly during the start of the training code. This is just a demo, there will not a saved dataset somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c3ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load a program from the disk.\n",
    "programs = json.load(open(\"seed_programs.json\"))\n",
    "program = (\"07065a\", programs[\"07065a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0eb70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you just need to print out what a program looks like.\n",
    "# program is serialized in the following format.\n",
    "\"\"\"\n",
    "[\n",
    "    [[synonym_left, synonym_right], 's', [first_op_left, first_op_right/second_op_left, second_op_right]],\n",
    "    [[second_logic_left, second_logic_right], '<logic_gate>'],\n",
    "    [third_logic_right, '<logic_gate>']\n",
    "]\n",
    "\"\"\"\n",
    "program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also visualize your program.\n",
    "visualize_program(program[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on this program, we can then sample factual inputs.\n",
    "sample_factual_inputs(program[1], all_vocab, synonyms_pairs, synonyms_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75df9c15",
   "metadata": {},
   "source": [
    "As you can see, we now sample inputs as well as the output conditioned on these inputs and the program. So far, we already have all the components to train a network to solve these programs. Let's finetune one and the get to the counterfactual dataset creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af9c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factual_input_instruction(program):\n",
    "    digit_to_word_mapping = {\n",
    "        0 : \"first\", \n",
    "        1 : \"second\", \n",
    "        2 : \"third\", \n",
    "        3 : \"fourth\",\n",
    "        4 : \"fifth\"\n",
    "    }\n",
    "    \n",
    "    first_same_or_not = \"the same\" if program[0][3][0] == \"==\" else \"not the same\"\n",
    "    second_same_or_not = \"the same\" if program[0][3][1] == \"==\" else \"not the same\"\n",
    "    second_part = f\"A is True if \"\\\n",
    "                  f\"the {digit_to_word_mapping[program[0][2][0]]} word \"\\\n",
    "                  f\"and the {digit_to_word_mapping[program[0][2][1]]} word are {first_same_or_not}, otherwise False.\"\n",
    "    third_part = f\"B is True if \"\\\n",
    "                  f\"the {digit_to_word_mapping[program[0][2][1]]} word \"\\\n",
    "                  f\"and the {digit_to_word_mapping[program[0][2][2]]} word are {second_same_or_not}, otherwise False.\"\n",
    "    \n",
    "    first_part = f\"C is True if \"\\\n",
    "                 f\"the {digit_to_word_mapping[program[0][0][0]]} word \"\\\n",
    "                 f\"and the {digit_to_word_mapping[program[0][0][1]]} word are synonyms, otherwise False.\"\n",
    "    \n",
    "    first_logic_gate = \"and\" if program[1][-1] == \"AND\" else \"or\"\n",
    "    second_logic_gate = \"and\" if program[-1][-1] == \"AND\" else \"or\"\n",
    "    \n",
    "    first_left_var = \"A\" if program[1][0][0] == 5 else \"B\" if program[1][0][0] == 6 else \"C\"\n",
    "    first_right_var = \"A\" if program[1][0][1] == 5 else \"B\" if program[1][0][1] == 6 else \"C\"\n",
    "    second_var = \"A\" if program[-1][0] == 5 else \"B\" if program[-1][0] == 6 else \"C\"\n",
    "    \n",
    "    fourth_part = f\"D is True if \"\\\n",
    "                  f\"{first_left_var} {first_logic_gate} {first_right_var} is True, otherwise False.\"\n",
    "    \n",
    "    fifth_part = f\"The output is True if \"\\\n",
    "                  f\"{second_var} {second_logic_gate} D is True, otherwise False.\"\n",
    "    \n",
    "    # test_demo = \"What is the output? \"\n",
    "    \n",
    "    program_str = \"\\n\".join([second_part, third_part, first_part, fourth_part, fifth_part])\n",
    "    return program_str\n",
    "\n",
    "\"\"\"\n",
    "To make this more compatible with HF, maybe let's \n",
    "make this as a seq to seq task where the input is \n",
    "a sentence of words, and the output is also a single\n",
    "word in this case.\n",
    "\"\"\"\n",
    "\n",
    "def prepare_factual_training_data_single_program(\n",
    "    program, \n",
    "    n_sample, \n",
    "    n_in_context_demo,\n",
    "    data_path=\".\",\n",
    "    input_trigger=\"INPUT\",\n",
    "    output_trigger=\"OUTPUT\",\n",
    "    program_uuid=None,\n",
    "    mode=\"I\" # [\"[I]nstructions\", \"[E]xplainations\", \"[N]umber\"]\n",
    "):\n",
    "    if mode == \"E\":\n",
    "        prompt_starter=\"Based on given word sets, deduce the \"\\\n",
    "                       \"hidden rule involving word equality and \"\\\n",
    "                       \"synonymy from given examples in the context \"\\\n",
    "                       \"and apply the deduced rule to the new input.\"\n",
    "    else:\n",
    "        prompt_starter=\"Based on the given rule involving word \"\\\n",
    "                       \"equality and synonymy for a given input, \"\\\n",
    "                       \"apply the rule to the new input.\"\n",
    "        task_instruction = sample_factual_input_instruction(program)\n",
    "    instruction_template = \"\"\"Instruction:\n",
    "%s\n",
    "\n",
    "Context:\n",
    "%s\n",
    "\n",
    "Response:\n",
    "%s\"\"\"\n",
    "    all_vocab, synonyms_pairs, synonyms_dict = fetch_metadata(data_path)\n",
    "    demo_sep=\"\\n\"\n",
    "    count = 0\n",
    "    examples = []\n",
    "    unique_hash_set = set([])\n",
    "    while len(examples) < n_sample:\n",
    "        if mode == \"E\":\n",
    "            demos = []\n",
    "            for _ in range(n_in_context_demo):\n",
    "                program, inputs, op_maps, value_maps = sample_factual_inputs(\n",
    "                    program, all_vocab, synonyms_pairs, synonyms_dict\n",
    "                )\n",
    "                input_words = [inputs[i] for i in range(len(inputs))]\n",
    "                input_sentence = \" \".join(input_words)\n",
    "                output_word = value_maps[f'op{len(inputs)}']\n",
    "                single_demo = f\"{input_trigger} {input_sentence} {output_trigger} {output_word}\"\n",
    "                demos += [single_demo]\n",
    "            program, inputs, op_maps, value_maps = sample_factual_inputs(\n",
    "                program, all_vocab, synonyms_pairs, synonyms_dict\n",
    "            )\n",
    "            test_words = [inputs[i] for i in range(len(inputs))]\n",
    "            test_sentence = \" \".join(test_words)\n",
    "            test_demo = f\"{input_trigger} {test_sentence} {output_trigger} \"\n",
    "            demos = \"\\n\".join(demos)\n",
    "            answers = value_maps[f'op{len(inputs)}']\n",
    "            question = instruction_template % (prompt_starter, demos, test_demo)\n",
    "            task_name = \"word_logic_E\"\n",
    "        elif mode == \"I\":\n",
    "            program, inputs, op_maps, value_maps = sample_factual_inputs(\n",
    "                program, all_vocab, synonyms_pairs, synonyms_dict\n",
    "            )\n",
    "            test_words = [inputs[i] for i in range(len(inputs))]\n",
    "            test_sentence = \" \".join(test_words)\n",
    "            test_demo = f\"{input_trigger} {test_sentence} {output_trigger} \"\n",
    "            answers = value_maps[f'op{len(inputs)}']\n",
    "            question = instruction_template % (prompt_starter, task_instruction, test_demo)\n",
    "            task_name = \"word_logic_I\"\n",
    "        elif mode == \"N\":\n",
    "            assert program_uuid is not None, \"The mode N requires a program UUID.\"\n",
    "            program, inputs, op_maps, value_maps = sample_factual_inputs(\n",
    "                program, all_vocab, synonyms_pairs, synonyms_dict\n",
    "            )\n",
    "            test_words = [inputs[i] for i in range(len(inputs))]\n",
    "            test_sentence = \" \".join(test_words)\n",
    "            test_demo = f\"{input_trigger} {test_sentence} {output_trigger} \"\n",
    "            answers = value_maps[f'op{len(inputs)}']\n",
    "            question = instruction_template % (prompt_starter, program_uuid, test_demo)\n",
    "            task_name = \"word_logic_N\"\n",
    "        else:\n",
    "            assert False, f\"The mode {mode} is unknown.\"\n",
    "\n",
    "        if f\"{question} {answers}\" not in unique_hash_set:\n",
    "            example = {\n",
    "                \"question\": question,\n",
    "                \"answers\": answers,\n",
    "                \"program\": program,\n",
    "                \"task_name\": task_name\n",
    "            }\n",
    "            examples += [example]\n",
    "            unique_hash_set.add(f\"{question} {answers}\")\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec74d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_factual_training_data_single_program(\n",
    "    program[1],\n",
    "    3, \n",
    "    10,\n",
    "    mode=\"I\",\n",
    "    program_uuid=program[0],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
